{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import gamma\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report , fbeta_score , confusion_matrix\n",
    "from matplotlib import pylab as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler ,EarlyStopping \n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = pd.read_excel('Dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.to_datetime(allData['EnglishDate'])\n",
    "allData['Year'] = date.dt.year\n",
    "allData['Month'] = date.dt.month\n",
    "allData['Day'] = date.dt.day\n",
    "allData['DayOfWeek'] = date.dt.day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2296 337\n"
     ]
    }
   ],
   "source": [
    "InsID = 38\n",
    "trainData_all = allData[allData['InstrumentID']==InsID].reset_index(drop=True)\n",
    "trainData_all = trainData_all.drop(['InstrumentID'] , axis = 1)\n",
    "trainData_all.fillna(0,inplace=True)\n",
    "neg, pos = np.bincount(trainData_all['Target1'])\n",
    "print(neg,pos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = []\n",
    "trainData = trainData_all[:trainData_all.shape[0]-10]\n",
    "Targets = trainData[['Target1']]\n",
    "trainData = trainData.drop(['Target1'] , axis = 1)\n",
    "print(trainData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData_seq = trainData.loc[:,['AdjustedClosePrice','AdjustedOpenPrice','AdjustedHighPrice','AdjustedLowPrice',\n",
    "'EMA12','EMA21','EMA26','EMA50','EMA100','EMA200','EMA260','EMA365','MACD','MACDSignal','MACDGap',\n",
    "'LNAdjustedClosePrice','LNAdjustedRealClosePrice','LNAdjustedOpenPrice','LNAdjustedHighPrice','LNAdjustedLowPrice',]]\n",
    "\n",
    "sc=StandardScaler()\n",
    "trainData_seq = sc.fit_transform(trainData_seq)\n",
    "print(trainData.shape)\n",
    "print(trainData_seq.shape)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder_input = []\n",
    "Decoder_input = []\n",
    "\n",
    "days = 30\n",
    "j = 0\n",
    "for i in range(days,np.size(trainData,0)):\n",
    "    Encoder_input.append(trainData_seq[i-days:i])\n",
    "    Decoder_input.append(trainData.loc[i ,['Year' , 'Month' , 'Day' , 'DayOfWeek']])\n",
    "    j += 1\n",
    "Target = np.asarray(Targets[days:])\n",
    "Encoder_input = np.asarray(Encoder_input).reshape(-1,days,trainData_seq.shape[1]).astype('float64')\n",
    "Conv_input = Encoder_input.reshape(-1,days,trainData_seq.shape[1],1).astype('float64')\n",
    "Decoder_input = np.asarray(Decoder_input).reshape(-1,1,4).astype('float64')\n",
    "print('trainData_seq shape:' , trainData_seq.shape)\n",
    "print('Encoder input shape:' , Encoder_input.shape)\n",
    "print('Conv input shape:' , Conv_input.shape)\n",
    "print('Decoder input shape:' , Decoder_input.shape)\n",
    "print('Target shape:' , Target.shape)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_encoder , test_data_encoder , train_data_decoder , test_data_decoder ,train_data_conv ,test_data_conv, train_target , test_target   = train_test_split(\n",
    "    Encoder_input,Decoder_input,Conv_input,Target,test_size = 0.3 , random_state = 42)\n",
    "print(train_data_encoder.shape)\n",
    "print(test_data_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.02\n",
    "    initial_learning_rate = 0.001\n",
    "    \n",
    "    return initial_learning_rate * np.exp(-k*epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = np.log([pos/neg])\n",
    "initializer_bias = tf.keras.initializers.Constant(bias)\n",
    "LSTM1Inputs = tfl.Input(shape=(np.size(train_data_encoder,1), np.size(train_data_encoder,2)))\n",
    "ConvInput = tfl.Input(shape=(np.size(train_data_conv,1), np.size(train_data_conv,2), np.size(train_data_conv,3)))\n",
    "LSTM1,_,_ = tfl.LSTM(64, return_state=True ,return_sequences=True,dropout=0.2)(LSTM1Inputs)\n",
    "Conv1 = tfl.Conv2D(30,(1,3),padding='valid')(ConvInput)\n",
    "LSTM2Inputs = tfl.Reshape((np.size(train_data_encoder,1),-1))(Conv1)\n",
    "LSTM2,_,_ =  tfl.LSTM(15, return_state=True ,return_sequences=True,dropout=0.2)(LSTM2Inputs)\n",
    "Conv2 = tfl.Conv2D(60,(1,5),padding='valid')(Conv1)\n",
    "LSTM3Inputs = tfl.Reshape((np.size(train_data_encoder,1),-1))(Conv2)\n",
    "LSTM3,_,_ = tfl.LSTM(15, return_state=True ,return_sequences=True,dropout=0.2)(LSTM3Inputs)\n",
    "EncoderInputs = tfl.concatenate([LSTM1,LSTM2,LSTM3],axis = 2)\n",
    "encoder = tfl.LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(EncoderInputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder = tfl.LSTM(256, return_sequences=True, return_state=True,dropout=0.2)\n",
    "decoderInputs = tfl.Input(shape=(np.size(train_data_decoder,1), np.size(train_data_decoder,2)))\n",
    "decoder_output, _, _ = decoder(decoderInputs,initial_state=encoder_states)\n",
    "Dense1 = tfl.Dense(100, activation='relu')(decoder_output)\n",
    "Drop1 = tfl.Dropout(0.2)(Dense1)\n",
    "Dense2 = tfl.Dense(30, activation='relu' )(Drop1)\n",
    "Dense3 = tfl.Dense(1, activation='sigmoid' , kernel_initializer=initializer_bias)(Dense2)\n",
    "\n",
    "early_stopper = EarlyStopping(patience=30, monitor='loss')\n",
    "\n",
    "Mod = tf.keras.Model([LSTM1Inputs ,ConvInput, decoderInputs], Dense3)\n",
    "Mod.compile(loss='binary_crossentropy', optimizer='adam' , metrics=['AUC'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "History = Mod.fit(\n",
    "    [train_data_encoder ,train_data_conv, train_data_decoder] ,\n",
    "    train_target ,\n",
    "    epochs = 300 ,\n",
    "    batch_size=500 ,\n",
    "    validation_split=0.1,\n",
    " callbacks=[\n",
    " LearningRateScheduler(lr_exp_decay, verbose=1),\n",
    " early_stopper,\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = np.floor(Mod.predict([test_data_encoder ,test_data_conv, test_data_decoder])+0.5).reshape(-1,1)\n",
    "ytrain = np.floor(Mod.predict([train_data_encoder ,train_data_conv, train_data_decoder])+0.5).reshape(-1,1)\n",
    "print(confusion_matrix(train_target.reshape(-1,1), ytrain))\n",
    "print(confusion_matrix(test_target.reshape(-1,1), ytest))\n",
    "print(classification_report(train_target.reshape(-1,1), ytrain))\n",
    "print(classification_report(test_target.reshape(-1,1), ytest))\n",
    "f2_test = fbeta_score(test_target.reshape(-1,1), ytest,beta=.2)\n",
    "f2_train = fbeta_score(train_target.reshape(-1,1), ytrain,beta=.2)\n",
    "\n",
    "print(f2_test)\n",
    "print(f2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.semilogy(history.epoch, history.history['loss'],\n",
    "                label='Train')\n",
    "  plt.semilogy(history.epoch, history.history['val_loss'],\n",
    "                label='Val ',\n",
    "               linestyle=\"--\")\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(History)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
